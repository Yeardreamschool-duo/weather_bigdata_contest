{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "시계열 앙상블.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "onKGJ7R2kpCR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import dstack\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import LSTM, GRU, Bidirectional\n",
        "from keras.models import load_model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import requests\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "pd.options.mode.chained_assignment = None\n",
        "tfd = tfp.distributions\n",
        "tfk = tf.keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/강릉_knn_train.csv', index_col = 'yyyymmddhhnn')"
      ],
      "metadata": {
        "id": "pw-AMCTykrML"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train['2020-01-01 00:00:00':'2020-05-31 23:50:00'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRm9YoMgEp4X",
        "outputId": "7ea3256c-28fe-48fc-b87f-ee8f697d515b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21888"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scale_cols = ['solarza','esr','uv']\n",
        "df_scaled = scaler.fit_transform(train[scale_cols])\n",
        "\n",
        "df_scaled = pd.DataFrame(df_scaled)\n",
        "df_scaled.columns = scale_cols"
      ],
      "metadata": {
        "id": "gLhGlK0ZlfaA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 25345\n",
        "\n",
        "dataset = df_scaled\n",
        "train = df_scaled[:-TEST_SIZE]\n",
        "test = df_scaled[-TEST_SIZE:]"
      ],
      "metadata": {
        "id": "z-IbSZXXl_ub"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(data, label, window_size):\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
        "        label_list.append(np.array(label.iloc[i+window_size]))\n",
        "    return np.array(feature_list), np.array(label_list)"
      ],
      "metadata": {
        "id": "CVIYNdnamFw4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gru(train, test, feature_cols, label_cols, window_size, units):\n",
        "\n",
        "    train_feature = train[feature_cols]\n",
        "    train_label = train[label_cols]\n",
        "\n",
        "    # train dataset\n",
        "    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n",
        "\n",
        "    # train, validation set 생성\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "\n",
        "    x_train.shape, x_valid.shape\n",
        "    # ((6086, 20, 4), (1522, 20, 4))\n",
        "\n",
        "    test_feature = test[feature_cols]\n",
        "    test_label = test[label_cols]\n",
        "\n",
        "    # test dataset (실제 예측 해볼 데이터)\n",
        "    test_feature, test_label = make_dataset(test_feature, test_label, window_size)\n",
        "    test_feature.shape, test_label.shape\n",
        "    # ((180, 20, 4), (180, 1))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units,input_shape=(train_feature.shape[1], train_feature.shape[2]), return_sequences=False ) )\n",
        "    model.add(Dropout(0.2))\n",
        "    # model.add(GRU(units = 64,return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(GRU(units = 32))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "    filename = os.path.join('./', 'gru.h5')\n",
        "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "    \n",
        "    GRU_history = model.fit(x_train, y_train, \n",
        "                        epochs=100, \n",
        "                        batch_size=16,\n",
        "                        validation_data=(x_valid, y_valid), \n",
        "                        callbacks=[early_stop, checkpoint])\n",
        "\n",
        "    # weight 로딩\n",
        "    model.load_weights(filename)\n",
        "\n",
        "    # prediction\n",
        "    train_pred = model.predict(train_feature)\n",
        "    test_pred = model.predict(test_feature)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    trainScore = math.sqrt(mean_squared_error(train_pred, train_label))\n",
        "    print('Train Score: %.3f RMSE' % (trainScore))\n",
        "    testScore = math.sqrt(mean_squared_error(test_pred, test_label))\n",
        "    print('Test Score: %.3f RMSE' % (testScore))\n",
        "    \n",
        "    visualizer(test_label, test_pred, \"GRU Prediction\")\n",
        "\n",
        "def bilstm(train, test, feature_cols, label_cols, window_size, units):\n",
        "\n",
        "\n",
        "    train_feature = train[feature_cols]\n",
        "    train_label = train[label_cols]\n",
        "\n",
        "    # train dataset\n",
        "    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n",
        "\n",
        "    # train, validation set 생성\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "\n",
        "    x_train.shape, x_valid.shape\n",
        "    # ((6086, 20, 4), (1522, 20, 4))\n",
        "\n",
        "    test_feature = test[feature_cols]\n",
        "    test_label = test[label_cols]\n",
        "\n",
        "    # test dataset (실제 예측 해볼 데이터)\n",
        "    test_feature, test_label = make_dataset(test_feature, test_label, window_size)\n",
        "    test_feature.shape, test_label.shape\n",
        "    # ((180, 20, 4), (180, 1))\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units = units,                             \n",
        "              return_sequences=False),\n",
        "              input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(Bidirectional(LSTM(units = 24,return_sequences=True)))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(Bidirectional(LSTM(units = 16)))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    #Compile model\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "  \n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "    filename = os.path.join('./', 'bilstm.h5')\n",
        "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "    lstm_history = model.fit(x_train, y_train, \n",
        "                        epochs=100, \n",
        "                        batch_size=32,\n",
        "                        validation_data=(x_valid, y_valid), \n",
        "                        callbacks=[early_stop, checkpoint])\n",
        "\n",
        "    # weight 로딩\n",
        "    model.load_weights(filename)\n",
        "\n",
        "    # 예측\n",
        "\n",
        "    train_pred = model.predict(train_feature)\n",
        "    test_pred = model.predict(test_feature)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    trainScore = math.sqrt(mean_squared_error(train_pred, train_label))\n",
        "    print('Train Score: %.3f RMSE' % (trainScore))\n",
        "    testScore = math.sqrt(mean_squared_error(test_pred, test_label))\n",
        "    print('Test Score: %.3f RMSE' % (testScore))\n",
        "\n",
        "    visualizer(test_label, test_pred, \"B-LSTM Prediction\")\n",
        "    \n",
        "def lstm(train, test, feature_cols, label_cols, window_size, units):\n",
        "\n",
        "    train_feature = train[feature_cols]\n",
        "    train_label = train[label_cols]\n",
        "\n",
        "    # train dataset\n",
        "    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n",
        "\n",
        "    # train, validation set 생성\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "\n",
        "    x_train.shape, x_valid.shape\n",
        "    # ((6086, 20, 4), (1522, 20, 4))\n",
        "\n",
        "    test_feature = test[feature_cols]\n",
        "    test_label = test[label_cols]\n",
        "\n",
        "    # test dataset (실제 예측 해볼 데이터)\n",
        "    test_feature, test_label = make_dataset(test_feature, test_label, window_size)\n",
        "    test_feature.shape, test_label.shape\n",
        "    # ((180, 20, 4), (180, 1))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units, \n",
        "                  input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
        "                  return_sequences=False)\n",
        "              )\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(64, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(32, return_sequences=False))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "    filename = os.path.join('./', 'lstm.h5')\n",
        "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "    lstm_history = model.fit(x_train, y_train, \n",
        "                        epochs=100, \n",
        "                        batch_size=16,\n",
        "                        validation_data=(x_valid, y_valid), \n",
        "                        callbacks=[early_stop, checkpoint])\n",
        "\n",
        "    # weight 로딩\n",
        "    model.load_weights(filename)\n",
        "\n",
        "    # 예측\n",
        "    train_pred = model.predict(train_feature)\n",
        "    test_pred = model.predict(test_feature)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    trainScore = math.sqrt(mean_squared_error(train_pred, train_label))\n",
        "    print('Train Score: %.3f RMSE' % (trainScore))\n",
        "    testScore = math.sqrt(mean_squared_error(test_pred, test_label))\n",
        "    print('Test Score: %.3f RMSE' % (testScore))\n",
        "\n",
        "    visualizer(test_label, test_pred, \"LSTM Prediction\")\n",
        "\n",
        "def load_all_models():\n",
        "    models = ['lstm', 'gru', 'bilstm']\n",
        "    #models = ['bilstm', 'gru']\n",
        "\n",
        "\n",
        "    all_models = []\n",
        "    for model in models:\n",
        "        filename = model + '.h5'\n",
        "        model = load_model(filename)\n",
        "        all_models.append(model)\n",
        "    return all_models\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(inputX):\n",
        "    stackX = None\n",
        "    models = load_all_models()\n",
        "    for model in models:\n",
        "      # make prediction\n",
        "      yhat = model.predict(inputX, verbose=0)\n",
        "      # stack predictions into [rows, members, probabilities]\n",
        "      if stackX is None:\n",
        "        stackX = yhat\n",
        "      else:\n",
        "        stackX = dstack((stackX, yhat))\n",
        "\n",
        "\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "    return stackX\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(inputX, inputy, window_size):\n",
        "\n",
        "\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(inputX)\n",
        "    # fit standalone model\n",
        "    model = KNeighborsRegressor(n_neighbors=3)\n",
        "    model.fit(stackedX, inputy[window_size:])\n",
        "    return model\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(inputX, inputy, window_size):\n",
        "\n",
        "    test_feature, test_label = make_dataset(inputX, inputy, window_size)\n",
        "    model = fit_stacked_model(test_feature, inputy, window_size)\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(test_feature)\n",
        "    # make a prediction\n",
        "    ensemble_pred = model.predict(stackedX)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    testScore = math.sqrt(mean_squared_error(ensemble_pred, test_label))\n",
        "    print('Test Score: %.3f RMSE' % (testScore))\n",
        "\n",
        "    visualizer(test_label, ensemble_pred, \"Ensemble Prediction\")\n",
        "\n",
        "\n",
        "\n",
        "def visualizer(test_label, pred, title):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.title('Comparison Between Actual Price and ' + title +' Price')\n",
        "    plt.ylabel('Price')\n",
        "    plt.xlabel('Date')\n",
        "    \n",
        "    plt.plot(test_label, color='r', label='Actual Price')\n",
        "    plt.plot(pred, color='b', label='Prediction Price')\n",
        "    \n",
        "    ax.grid(True)\n",
        "    fig.tight_layout()\n",
        "    plt.legend()\n",
        "    plt.savefig(title + '.svg', format='svg')\n",
        "    plt.show()\n",
        "\n",
        "def run_all(train, test, feature_cols, label_cols, window_size, units):\n",
        "    lstm(train, test, feature_cols, label_cols, window_size, units)\n",
        "    gru(train, test, feature_cols, label_cols, window_size, units)\n",
        "    bilstm(train, test, feature_cols, label_cols, window_size, units)\n",
        "    stacked_prediction(test[feature_cols], test[label_cols], window_size)"
      ],
      "metadata": {
        "id": "V2cO4-eBmK6q"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_only = ['solarza','esr']\n",
        "feature_cols = x_only\n",
        "label_cols = ['uv']\n",
        "\n",
        "run_all(train, test, feature_cols, label_cols, 10, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LJBKMxBnOQJ",
        "outputId": "aaa5cbb5-c917-46ee-d56c-57d3af2f560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0062\n",
            "Epoch 1: val_loss improved from inf to 0.00537, saving model to ./lstm.h5\n",
            "3996/3996 [==============================] - 121s 30ms/step - loss: 0.0062 - val_loss: 0.0054\n",
            "Epoch 2/100\n",
            "3994/3996 [============================>.] - ETA: 0s - loss: 0.0056\n",
            "Epoch 2: val_loss improved from 0.00537 to 0.00523, saving model to ./lstm.h5\n",
            "3996/3996 [==============================] - 120s 30ms/step - loss: 0.0056 - val_loss: 0.0052\n",
            "Epoch 3/100\n",
            "3996/3996 [==============================] - ETA: 0s - loss: 0.0056\n",
            "Epoch 3: val_loss did not improve from 0.00523\n",
            "3996/3996 [==============================] - 119s 30ms/step - loss: 0.0056 - val_loss: 0.0054\n",
            "Epoch 4/100\n",
            "3996/3996 [==============================] - ETA: 0s - loss: 0.0055\n",
            "Epoch 4: val_loss improved from 0.00523 to 0.00517, saving model to ./lstm.h5\n",
            "3996/3996 [==============================] - 121s 30ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 5/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0055\n",
            "Epoch 5: val_loss did not improve from 0.00517\n",
            "3996/3996 [==============================] - 120s 30ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 6/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0055\n",
            "Epoch 6: val_loss improved from 0.00517 to 0.00516, saving model to ./lstm.h5\n",
            "3996/3996 [==============================] - 121s 30ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 7/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0055\n",
            "Epoch 7: val_loss did not improve from 0.00516\n",
            "3996/3996 [==============================] - 120s 30ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 8/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0055\n",
            "Epoch 8: val_loss did not improve from 0.00516\n",
            "3996/3996 [==============================] - 121s 30ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 9/100\n",
            "3995/3996 [============================>.] - ETA: 0s - loss: 0.0055\n",
            "Epoch 9: val_loss did not improve from 0.00516\n",
            "3996/3996 [==============================] - 125s 31ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 10/100\n",
            "3996/3996 [==============================] - ETA: 0s - loss: 0.0055\n",
            "Epoch 10: val_loss did not improve from 0.00516\n",
            "3996/3996 [==============================] - 121s 30ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 11/100\n",
            " 805/3996 [=====>........................] - ETA: 1:29 - loss: 0.0054"
          ]
        }
      ]
    }
  ]
}